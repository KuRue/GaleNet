#!/usr/bin/env python3
"""CLI for evaluating baseline and model forecasts across storms.

This script accepts a list of storm identifiers, loads the corresponding
tracks via :class:`HurricaneDataPipeline`, and evaluates a suite of
baseline models.  Optionally, forecasts generated by
``GaleNetPipeline`` are evaluated alongside the baselines.  Results are
printed and can be exported to either CSV or JSON format.
"""

from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import List, Sequence, Tuple
import sys

import numpy as np
import pandas as pd

# Ensure local ``src`` package is importable when running as a script
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

from galenet import GaleNetPipeline, HurricaneDataPipeline
from galenet.evaluation.baselines import evaluate_baselines


def _fetch_tracks(
    pipeline: HurricaneDataPipeline, storm_ids: Sequence[str]
) -> List[Tuple[str, np.ndarray]]:
    """Fetch tracks for ``storm_ids`` using ``pipeline``.

    Returns a list of ``(storm_id, track_array)`` tuples where each
    ``track_array`` has columns ``(lat, lon, intensity)``.
    """

    storms: List[Tuple[str, np.ndarray]] = []
    for storm_id in storm_ids:
        logging.info("Fetching track for storm %s", storm_id)
        data = pipeline.load_hurricane_for_training(storm_id, include_era5=False)
        track_df = data["track"]
        if "timestamp" in track_df.columns:
            track_df = track_df.sort_values("timestamp")
        track = track_df[["latitude", "longitude", "max_wind"]].to_numpy()
        storms.append((storm_id, track))
    return storms


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Evaluate baseline and model forecasts",
    )
    parser.add_argument(
        "storm_ids",
        nargs="+",
        help="Storm identifiers to evaluate (e.g. AL012019 AL022019)",
    )
    parser.add_argument("--history", type=int, default=3, help="Number of history steps")
    parser.add_argument("--forecast", type=int, default=2, help="Number of forecast steps")
    parser.add_argument(
        "--model-config",
        type=Path,
        default=None,
        help="Optional path to config for running GaleNetPipeline forecasts",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="model",
        help="Name to use for reporting model metrics",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional path to write summary (CSV or JSON)",
    )
    parser.add_argument(
        "--no-model",
        action="store_true",
        help="Skip running GaleNetPipeline model forecasts",
    )
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)

    try:
        data_pipeline = HurricaneDataPipeline(
            str(args.model_config) if args.model_config else None
        )
        storms_with_ids = _fetch_tracks(data_pipeline, args.storm_ids)
        tracks: Sequence[np.ndarray] = [t for _, t in storms_with_ids]

        model_forecasts = None
        if not args.no_model:
            pipeline = GaleNetPipeline(
                str(args.model_config) if args.model_config else None
            )
            model_forecasts = []
            for storm_id, track in storms_with_ids:
                logging.info("Running model for storm %s", storm_id)
                history = track[: args.history]
                df_hist = pd.DataFrame(
                    history, columns=["latitude", "longitude", "max_wind"]
                )
                preds = pipeline.model.predict(df_hist, args.forecast, 1)
                model_forecasts.append(
                    preds[["latitude", "longitude", "max_wind"]].to_numpy()
                )

        results = evaluate_baselines(
            tracks,
            args.history,
            args.forecast,
            model_forecasts=model_forecasts,
            model_name=args.model_name,
        )

        df = pd.DataFrame(results).T
        print(df.to_string(float_format=lambda x: f"{x:.3f}"))
        if args.output is not None:
            if args.output.suffix == ".json":
                df.to_json(args.output, orient="index", indent=2)
            else:
                df.to_csv(args.output)

        if df.isna().any().any():
            raise RuntimeError("NaN encountered in evaluation metrics")

    except Exception as exc:  # pragma: no cover - CLI failure path
        logging.error("Evaluation failed: %s", exc)
        raise SystemExit(1) from exc


if __name__ == "__main__":
    main()

