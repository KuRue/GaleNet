#!/usr/bin/env python3
"""CLI for evaluating baseline and model forecasts across storms.

This script accepts a list of storm identifiers, loads the corresponding
tracks via :class:`HurricaneDataPipeline`, and evaluates a suite of
baseline models.  Optionally, forecasts generated by
``GaleNetPipeline`` are evaluated alongside the baselines.  Results are
printed and can be exported to either CSV or JSON format.
"""

from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import Dict, List, Sequence, Tuple
import sys

import numpy as np
import pandas as pd

# Ensure local ``src`` package is importable when running as a script
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

from galenet import GaleNetPipeline, HurricaneDataPipeline
from galenet.evaluation.baselines import run_baselines
from galenet.evaluation.metrics import compute_metrics


def _fetch_tracks(
    pipeline: HurricaneDataPipeline, storm_ids: Sequence[str]
) -> List[Tuple[str, np.ndarray]]:
    """Fetch tracks for ``storm_ids`` using ``pipeline``.

    Returns a list of ``(storm_id, track_array)`` tuples where each
    ``track_array`` has columns ``(lat, lon, intensity)``.
    """

    storms: List[Tuple[str, np.ndarray]] = []
    for storm_id in storm_ids:
        logging.info("Fetching track for storm %s", storm_id)
        data = pipeline.load_hurricane_for_training(storm_id, include_era5=False)
        track_df = data["track"]
        if "timestamp" in track_df.columns:
            track_df = track_df.sort_values("timestamp")
        track = track_df[["latitude", "longitude", "max_wind"]].to_numpy()
        storms.append((storm_id, track))
    return storms


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Evaluate baseline and model forecasts",
    )
    parser.add_argument(
        "storm_ids",
        nargs="+",
        help="Storm identifiers to evaluate (e.g. AL012019 AL022019)",
    )
    parser.add_argument("--history", type=int, default=3, help="Number of history steps")
    parser.add_argument("--forecast", type=int, default=2, help="Number of forecast steps")
    parser.add_argument(
        "--model-config",
        type=Path,
        default=None,
        help="Optional path to config for running GaleNetPipeline forecasts",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="model",
        help="Name to use for reporting model metrics",
    )
    parser.add_argument(
        "--model",
        action="append",
        default=[],
        help="Model specification as NAME=CONFIG. Repeat for multiple models",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional path to write summary (CSV or JSON)",
    )
    parser.add_argument(
        "--details",
        type=Path,
        default=None,
        help="Optional path to write per-storm metrics (CSV or JSON)",
    )
    parser.add_argument(
        "--no-model",
        action="store_true",
        help="Skip running GaleNetPipeline model forecasts",
    )
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)

    try:
        data_pipeline = HurricaneDataPipeline(
            str(args.model_config) if args.model_config else None
        )
        storms_with_ids = _fetch_tracks(data_pipeline, args.storm_ids)

        models: List[Tuple[str, GaleNetPipeline]] = []
        if args.model:
            for spec in args.model:
                if "=" not in spec:
                    raise ValueError(f"Invalid model specification: {spec}")
                name, cfg = spec.split("=", 1)
                models.append((name, GaleNetPipeline(cfg if cfg else None)))
        elif not args.no_model:
            models.append(
                (
                    args.model_name,
                    GaleNetPipeline(
                        str(args.model_config) if args.model_config else None
                    ),
                )
            )

        records: List[Dict[str, float]] = []
        for storm_id, track in storms_with_ids:
            history = track[: args.history]
            truth = track[args.history : args.history + args.forecast]
            forecasts = run_baselines(history, args.forecast)
            for name, pred in forecasts.items():
                results = compute_metrics(
                    pred[:, :2],
                    truth[:, :2],
                    pred[:, 2],
                    truth[:, 2],
                )
                rec: Dict[str, float] = {"storm": storm_id, "forecast": name}
                rec.update(results)
                records.append(rec)

            for model_name, pipeline in models:
                try:
                    df_hist = pd.DataFrame(
                        history, columns=["latitude", "longitude", "max_wind"]
                    )
                    preds = pipeline.model.predict(df_hist, args.forecast, 1)
                    pred = preds[["latitude", "longitude", "max_wind"]].to_numpy()
                except Exception as exc:
                    logging.error(
                        "Model %s failed on storm %s: %s", model_name, storm_id, exc
                    )
                    pred = np.full((args.forecast, 3), np.nan)

                results = compute_metrics(
                    pred[:, :2],
                    truth[:, :2],
                    pred[:, 2],
                    truth[:, 2],
                )
                rec = {"storm": storm_id, "forecast": model_name}
                rec.update(results)
                records.append(rec)

        per_storm_df = pd.DataFrame.from_records(records).set_index([
            "storm",
            "forecast",
        ])
        summary_df = per_storm_df.groupby("forecast").mean()

        print("Per-storm metrics:")
        print(per_storm_df.to_string(float_format=lambda x: f"{x:.3f}"))
        print("\nSummary:")
        print(summary_df.to_string(float_format=lambda x: f"{x:.3f}"))

        if args.details is not None:
            if args.details.suffix == ".json":
                per_storm_df.to_json(args.details, orient="index", indent=2)
            else:
                per_storm_df.to_csv(args.details)

        if args.output is not None:
            if args.output.suffix == ".json":
                summary_df.to_json(args.output, orient="index", indent=2)
            else:
                summary_df.to_csv(args.output)

        if per_storm_df.isna().any().any() or summary_df.isna().any().any():
            raise RuntimeError("NaN encountered in evaluation metrics")

    except Exception as exc:  # pragma: no cover - CLI failure path
        logging.error("Evaluation failed: %s", exc)
        raise SystemExit(1) from exc


if __name__ == "__main__":
    main()

